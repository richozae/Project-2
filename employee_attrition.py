# -*- coding: utf-8 -*-
"""Employee_Attrition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1deBSENr3J_qSgLZLSg3wxiBtyHJeLv0P
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/progresproject/Employee_Attrition.csv')

print(df)

df.head()

df.tail()

df.info()

df.describe()

df.columns

df.shape

df.isna().sum()

duplicates=df.duplicated()
df[duplicates]

df.nunique()

#exploration of the attrition column
df.hist(figsize=(20,20), color='green')

plt.bar(df['Attrition'].unique(), df['Attrition'].value_counts(), color=['orange', 'blue']);

# Attrition by Age
plt.figure(figsize=(9,3))
sns.countplot(x='Age', hue='Attrition', data=df,hue_order=['Yes','No'],palette=['orange','blue']);

# Attrition by Business Travel
plt.figure(figsize=(12,4))
sns.countplot(x='BusinessTravel', hue='Attrition', data=df,hue_order=['Yes','No'],palette=['orange','blue']);

# Attrition by Department
plt.figure(figsize=(12,4))
sns.countplot(x='Department', hue='Attrition', data=df,hue_order=['Yes','No'],palette=['orange','blue']);

# Attrition by DistanceFromHome
plt.figure(figsize=(12,4))
sns.countplot(x='DistanceFromHome', hue='Attrition', data=df,hue_order=['Yes','No'],palette=['orange','blue']);

# Attrition for all the columns
for i, col in enumerate(df.drop(['Attrition','EmployeeCount','EmployeeNumber','DailyRate','StandardHours','HourlyRate','MonthlyIncome','MonthlyRate'], axis=1).columns):
    plt.figure(i,figsize=(12,4))
    sns.countplot(x=col, hue='Attrition', data=df,hue_order=['Yes','No'],palette=['orange','blue']);

"""Clean the Data"""

def wrangle(filepath):
    df = pd.read_csv(filepath)

    columns_to_drop = ['EmployeeCount', 'EmployeeNumber', 'DailyRate', 'StandardHours', 'Over18']
    df.drop(columns=columns_to_drop, inplace=True)

    df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})

    df['OverTime'] = df['OverTime'].map({'Yes': 1, 'No': 0})

    df['BusinessTravel'] = df['BusinessTravel'].map({'Travel_Rarely': 1, 'Travel_Frequently': 2, 'Non-Travel': 0})

    print(df.dtypes)

    return df

data_filepath = '/content/drive/My Drive/Colab Notebooks/progresproject/Employee_Attrition.csv'
cleaned_df = wrangle(data_filepath)

print(cleaned_df.head())

"""Split the Data"""

X = cleaned_df.drop('Attrition', axis=1)
y = cleaned_df['Attrition']

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder , OrdinalEncoder
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

X_train_under, y_train_under = RandomUnderSampler(random_state=42).fit_resample(X_train, y_train)

X_train_over, y_train_over = RandomOverSampler(random_state=42).fit_resample(X_train, y_train)

"""Methods"""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

categorical_cols = ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']
ordinal_cols = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction',
                'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'WorkLifeBalance']
numerical_cols = ['Age','BusinessTravel', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',
                  'PercentSalaryHike', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany',
                  'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']

from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

# Kolom-kolom sesuai dengan dataset Anda
categorical_cols = ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']
ordinal_cols = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel',
 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel']
numerical_cols = ['Age', 'BusinessTravel', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome',
 'PercentSalaryHike', 'TotalWorkingYears', 'TrainingTimesLastYear',
 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']

# Create transformers for each type of feature
categorical_transformer = make_pipeline(
    OneHotEncoder(drop='first', sparse_output=False)  # Use drop='first' to handle multicollinearity
)


ordinal_transformer = make_pipeline(
    OrdinalEncoder()
)

numerical_transformer = make_pipeline(
    StandardScaler()
)

# Create a column transformer to apply the appropriate transformations to each column
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols),
        ('ord', ordinal_transformer, ordinal_cols)
    ])

# Combine the preprocessing steps and logistic regression into a single pipeline
lr = make_pipeline(
    preprocessor,
    LogisticRegression(random_state=42)
)

# Combine the preprocessing steps and random forest into a single pipeline
rf = make_pipeline(
    preprocessor,
    RandomForestClassifier(random_state=42)
)

# Combine the preprocessing steps and decision tree into a single pipeline
dt = make_pipeline(
    preprocessor,
    DecisionTreeClassifier(random_state=42)
)

# Combine the preprocessing steps and KNN into a single pipeline
knn = make_pipeline(
    preprocessor,
    KNeighborsClassifier()
)

pipelines = [lr, rf, dt, knn]
for pipe in pipelines:
    scores = cross_val_score(pipe, X_train_under, y_train_under, cv=5, scoring='accuracy')
    print(f'{pipe.steps[-1][1]}: {scores.mean():.4f} +/- {scores.std():.4f}')

for pipe in pipelines:
    scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy')
    print(f'{pipe.steps[-1][1]}: {scores.mean():.4f} +/- {scores.std():.4f}')

for pipe in pipelines:
    scores = cross_val_score(pipe, X_train_over, y_train_over, cv=5, scoring='accuracy',error_score='raise')
    print(f'{pipe.steps[-1][1]}: {scores.mean():.4f} +/- {scores.std():.4f}')

model = lr.fit(X_train, y_train)
y_pred = model.predict(X_test)
model.score(X_test, y_test)

model = rf.fit(X_train, y_train)
y_pred = model.predict(X_test)
model.score(X_test, y_test)

model = dt.fit(X_train, y_train)
y_pred = model.predict(X_test)
model.score(X_test, y_test)

model = knn.fit(X_train, y_train)
y_pred = model.predict(X_test)
model.score(X_test, y_test)

rf_better = GridSearchCV(
    rf,
    param_grid={
        'randomforestclassifier__n_estimators': [100, 200, 300],
        'randomforestclassifier__max_depth': [None, 5, 10, 15],
        'randomforestclassifier__min_samples_split': [2, 5, 10],
        'randomforestclassifier__min_samples_leaf': [1, 2, 5]
    },
    cv=5,
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)

rf_better.fit(X_train_over, y_train_over)

rf_better.best_params_

rf_better.score(X_test, y_test)

lr_better = GridSearchCV(
    lr,
    param_grid={
        'logisticregression__C': [0.1, 1, 10, 100],
        'logisticregression__solver': ['lbfgs', 'liblinear']
    },
    cv=5,
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)

lr_better.fit(X_train, y_train)

lr_better.best_params_

lr_better.score(X_test, y_test)

from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

confusion_matrix(y_test, lr_better.predict(X_test))

final_model = make_pipeline(
    preprocessor,
    LogisticRegression(C=1,solver='liblinear',random_state=42)
)

final_model.fit(X_train, y_train)

ConfusionMatrixDisplay.from_estimator(final_model,X_test,y_test)

final_model.score(X_test, y_test)

# the top 10 features that are most important in predicting attrition and plot it
importances = final_model.named_steps['logisticregression'].coef_[0]
features = final_model.named_steps['columntransformer'].get_feature_names_out()
features = np.append(features, numerical_cols)

# Check if the length of importances matches the length of features
if len(importances) < len(features):
    # If there are missing feature importances, set them to zero
    missing_importances = np.zeros(len(features) - len(importances))
    importances = np.concatenate((importances, missing_importances))

feat_importances = pd.Series(importances, index=features)
feat_importances.nlargest(10).plot(kind='barh');

top_10_importances = feat_importances.nlargest(10)

# Plot Pie Chart
plt.figure(figsize=(8, 8))
plt.pie(top_10_importances, labels=top_10_importances.index, autopct='%1.1f%%', startangle=140)
plt.title("Top 10 Feature Importances")
plt.show()